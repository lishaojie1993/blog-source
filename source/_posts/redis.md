---
title: 分布式缓存-Redis
date: 2019-03-12 23:46:44
categories: Java
tags:
  - 分布式
  - redis
top_img: https://tva1.sinaimg.cn/large/006tNbRwgy1gash4k2x0uj31900u0b1g.jpg
---

## redis单线程模型原理剖析？

redis基于reactor模式开发了网络事件处理器，这个处理器叫做文件事件处理器——file event handler。这个文件事件处理器是单线程的，所以redis才叫做单线程模型。redis采用IO多路复用机制同时监听多个socket，根据socket上的事件来选择对应的事件处理器来处理这个事件。

## redis为什么这么快？

1. 纯内存操作。
2. 核心是基于非阻塞的IO多路复用机制。
3. 单线程——避免了多线程的频繁上下文切换问题。
4. Resp协议。<!--more-->

## 使用redis有什么缺点？

- 缓存和数据库双写一致性问题
- 缓存雪崩、缓存击穿
- 缓存的并发竞争

## redis支持的数据类型和使用场景

1. string

   最基本的类型，普通的set和get，做简单的key-value缓存。

2. hash

   类似map的一种结构，特别适合用存储对象，可以仅仅修改这个对象中的某个字段。

3. list

   有序列表，这个是可以玩出很多花样的：

   比如可以基于redis的list实现简单的高性能分页，类似于微博的那种一直下拉。

   还可以搞个简单的消息队列，从list头进去，从list尾巴取出来。

4. set

   无序集合，自动去重。

   如果某个系统部署在多台机器上，可以基于redis实现全局的set去重。

   还可以基于set玩交集，并集，差集的操作，比如利用交集，可以查看两个人的粉丝列表中的共同好友。

5. sorted set

   有序集合，自动去重。

   这个也可以玩很多花样，写数据进去的时候给一个分数，自动根据分数排序，可以自定义排序规则。

   比如想根据数据的时间排序，那么在写入数据的时候把时间作为分数，这样就会按时间排序了。

   排行榜：将每个用户以及对应的分数写入进去，会自动排序，可以查看前几名的结果。

   zadd board 85 jack

   zadd board 72 tom

   zadd board 96 jerry

   zrevrange board 0 2：可以查看前三名的排序结果

   zrank board tom：返回3，意思是tom排名第3

------

## redis的过期策略有哪些

**定期删除+惰性删除**

所谓**定期删除**，指的是redis默认每隔100ms就随机抽取一些过期时间的key，检测是否过期，如果过期就删除。注意：这里redis不是遍历所有过期的key（CPU负载太高），而是随机抽取。

所谓**惰性删除**，指的是定期删除可能会导致很多过期的key到了时间也没被删除，所以在获取某个key的时候，redis会查一下这个key是否过期，如果过期了此时就会删除，不会返回结果。

产生问题：如果定期删除漏掉了很多过期key没删，惰性删除是用到的时候才删，如果过期了并且一直没用到就会导致有大量的过期key堆积，这时候就需要走**内存淘汰机制**了。

1. no-enviction（驱逐）：禁止驱逐数据，再写入会报错。**（默认，应该没人用）**
2. allkeys-lru：从数据集中挑选最近最少使用的数据淘汰。**（推荐使用）**
3. allkeys-random：从数据集中任意选择数据淘汰。
4. volatile-lru：从已设置过期时间的数据集中挑选最近最少使用的数据淘汰。
5. volatile-ttl：从`已设置过期时间的`数据集中挑选`将要过期的`数据淘汰。
6. volatile-random：从已设置过期时间的数据集中任意选择数据淘汰。

------

## 怎么保证redis的高并发&高可用？

redis高并发：采用**主从架构**，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万QPS，多从用来查询数据，多个从实例可以提供每秒上10万的QPS。如果redis高并发的同时还需要容纳大量的数据：几十G甚至几百G的数据，这样的话就需要采用redis集群了，还能提供每秒几十万的读写并发。

redis高可用：如果做主从架构部署，其实加上**哨兵**就可以了，任何一个实例宕机，都会自动切换。

### redis的主从复制

主从架构 -> 读写分离 -> 水平扩容支撑10+的**读QPS**

首先考虑读写分离，做成主从架构，一主多从，主负责写，并且将数据同步到其他slave节点，从节点负责读，所有的读请求都走从节点。好处是可以水平扩容，就是说如果QPS再增加，只需要继续增加slave就可以了。 

#### 主从复制的核心原理

当启动一个slave的时候，该节点会发送一个PSYNC命令给master，如果这是slave的重新连接，master仅仅会发送给slave部分缺少的数据进行**增量复制**，如果这是slave第一次连接master，会触发一次**全量复制**。

官方解释：

1. slave启动时，仅仅保存master的host和ip（redis.conf中配置的），此时复制流程还没开始。
2. slave内部有个定时任务，每秒都会check是否有新的master要连接和复制，如果发现则建立网络连接。
3. slave发送ping给master，如果master配置了requirepass，那么slave必须发送masterauth口令过去认证。
4. master第一次执行全量复制，将所有数据发送给slave。
5. master后续还会持续将写命令异步发送给slave。

全量复制：开始full resynchronized的时候，master会启动一个后台线程，生成一份RDB快照文件，同时还会将从客户端收到的所有写命令缓存在内存中。RDB文件生成完毕以后，master会将这个RDB发送给slave，slave会先写入本地磁盘，然后再从本地磁盘加载到内存中，接着master会将内存中缓存的写命令发送给slave，slave来同步这些数据。

增量复制：master根据slave发送的psync中的offset，在backlog中查找到部分丢失的数据，发送给slave。

#### 主从复制的断点续传

从redis2.8开始就支持主从复制的断点续传了。在主从复制过程中，如果网络连接断掉了，可以接着上次复制的地方继续复制，而不是重新开始复制。master会在内存中维护一个backlog，master和slave都会保存一个复制数据的replica offset和一个master run id，offset就是保存在backlog中的。如果master和slave网络中断了，slave会让master从上次的replica offset开始继续复制。但是如果没有找到对应的offset，那么就会执行一次resynchronized。

官方解释：

1. master和slave都会维护一个offset，slave每秒都会上报自己的offect给master，master记录在backlog中，这样才能知道双方数据是否一致。
2. master还会维护一个backlog文件，默认是1M大小，给slave复制数据时也会记录在backlog中，主要是用来做全量复制中断后的增量复制的。
3. master重启或者加载了之前的RDB数据是会变的，run id也会变，所以slave需要根据不同的run id区分，如果run id不同就需要做全量复制。
4. 从节点使用psync从master进行复制，发送psync runid offset到master。master会根据自身的情况返回相应的信息，可能是FULLRESYNC runid offset触发全量复制，也可能是CONTINUE触发增量复制。

#### 无磁盘化复制

在redis的配置文件中开启无磁盘化复制以后，master会在内存中直接创建rdb文件然后发送给slave，不会保存在本地磁盘。这里不建议开启，开启也很简单，主要涉及到两个参数：repl-diskless-sync no 默认是no，改成yes就可以了，repl-diskless-sync-delay 5 默认是延迟5s在开始复制，因为需要等待更多的slave重新连接。

#### 过期key处理

slave不会过期key，只会等待master过期key。

如果master过期了一个key，或者通过LRU淘汰了一个key，master会模拟一条del命令发送给slave。

#### redis的心跳检测机制

在命令传播阶段，slave每隔一秒向master发送一个心跳，主要用来检测双方的网络连接状态。

#### redis的持久化

如果采用了主从架构，建议必须开启master node的持久化。

不建议用slave node作为master node的热备，因为那样的话如果关掉master的持久化（RDB和AOF都关闭）可能在master宕机重启的时候数据是空的，然后从节点一复制，slave node的数据也丢了。

即使slave node可以自动接管master node，也可能哨兵还没有检测到master failure，master node就重启了，还是可能导致上面的所有slave node数据被清空的故障。

**持久化方式RDB和AOF**

- RDB持久化机制：通过快照的方式，对redis中的数据进行周期性的持久化。
- AOF持久化机制：通过记录写命令，以append-only模式写入到日志文件中，redis重启时重新构建。

如果同时开启了RDB和AOF两种持久化机制，在redis重启时会使用AOF来构建数据，因为AOF数据更加完整。

如果我们想要redis仅仅作为纯内存的缓存来用，可以关掉RDB和AOF持久化机制。

**RDB的优点**

1. RDB会生成多个数据文件，非常适合做冷备，可以上传到云盘定期维护。
2. RDB对redis的性能影响非常小，定期把数据写入到磁盘，使redis保持高性能。
3. 通过RDB数据文件来做数据恢复更加快速，直接把文件加载到内存即可。

**RDB的缺点**

1. 最大的缺点就是可能造成部分数据丢失。由于RDB是定期备份，可能每隔5分钟甚至更久，如果redis突然宕机，可能会丢失部分数据，所以RDB不适合作为第一优先的恢复方案。
2. RDB每次执行快照生成数据文件的时候，如果数据文件特别大，可能会导致redis对客户提供的服务暂停数秒，所以不要让RDB备份的间隔太长，否则每次生成的文件太大，影响redis本身的性能。

**AOF的优点**

1. AOF可以更好的保护数据不丢失，一般AOF会每隔一秒记录一次，所以最多丢失1秒的数据。
2. AOF日志以append-only模式写入，没有磁盘寻址开销，写入性能高，且文件不易破损。
3. AOF日志文件过大的时候，出现后台重写操作也不会影响reids客户端的读写效率。
4. AOF特别合适误删除的紧急恢复，比如输入了flushall清空了数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall删除，然后再将AOF文件放回去，就可以自动恢复。

**AOF的缺点**

1. 最大的缺点就是做数据恢复的时候会比较慢，做冷备和定期备份不方便，需要手写复杂脚本。
2. 对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大。
3. AOF开启后，支持的QPS会比RDB低，因为每秒一次fsync，不过性能还是很高的。
4. 如果想保证一条数据都不丢，也是可以的，设置成每写入一条数据就fsync一次，不过性能会大降。
5. AOF相比于RDB更加脆弱一些，恢复数据后可能导致跟原来不一样，容易产生bug。

**RDB和AOF到底如何选择**

- 不要仅仅使用RDB，因为那样会导致丢失很多数据。
- 也不要仅仅使用AOF，因为AOF不适合做冷备，恢复数据比较慢还容易产生bug。
- 所以综合使用AOF和RDB两种持久化机制，用AOF保证数据不丢失，作为数据恢复的第一选择；用RDB来做不同程度的冷备，在AOF文件都丢失或者损坏不可用的时候，还可以使用RDB快照来进行数据恢复。

------

### redis的哨兵机制

sentinal，中文名是哨兵，是redis集群架构中非常重要的一个组件，主要功能如下：

1. 集群监控：负责监控redis master和slave进程是否正常工作。
2. 消息通知：如果某个redis实例有故障，哨兵负责发送消息给管理员报警。
3. 故障转移：如果master挂掉了，会自动转移到slave上。
4. 配置中心：如果故障转移发生了，把新的master地址通知到客户端。

哨兵本身也是分布式的，作为一个哨兵集群在运行，互相协同工作。

1. 故障转移时，判断一个master是否宕机，需要大部分的哨兵同意才行，涉及到分布式选举。
2. 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，保证了系统的高可用性。

#### 哨兵的核心知识

1. 哨兵至少需要3个实例，来保证自己的健壮性。
2. 哨兵+redis主从的部署架构，是不会保证数据零丢失的，只能保证redis集群的高可用。
3. 对于哨兵+redis主从的复杂架构，尽量在测试和生产环境都进行充足的测试和演练。

**为什么redis哨兵集群少于3个节点无法正常工作**

如果哨兵集群只有两个节点，两个哨兵的majority=2，其中master所在的机器宕机了，这时只剩下一个哨兵，哨兵切换故障需要满足大多数哨兵同意原则，此时没有majority来运行执行故障转移，所以两个哨兵节点不能工作。

#### sdown和odown转换机制？

master宕机有sdown和odown两种失败状态

- sdown是主观宕机，就一个哨兵如果自己觉得一个master宕机了，就是主观宕机。
- odown是客观宕机，如果quorum数量的哨兵都觉得一个master宕机了，就是客观宕机。
- sdown达成的条件很简单，如果一个哨兵ping一个master，超过了is-master-after-milliseconds指定的毫秒数之后，就主观认为master宕机了
- sdown到odown的转换条件也很简单，如果一个哨兵在指定时间内，收到了quorum指定数量的其他哨兵也认为那个master是sdown了，那么就认为是odown了，也就是客观认为master宕机。

#### 哨兵和slave集群的自动发现机制

哨兵互相之间的发现，是通过redis的pub/sub系统实现的，每个哨兵都会往\_sentinel\_:hello channel里发送一个消息，内容是自己的host、ip和runid，还有对这个master的监控配置。每个哨兵也会去监听自己监控的master+slave对应的\_sentinel\_:hello channel，然后去感到到同样在监听这个master+slave的其他哨兵的存在，每个哨兵还会跟其他哨兵交换对master的监控配置，互相进行监控配置的同步。

#### slave-master选举算法

如果一个master被认为odown了，而且majority哨兵都允许了主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个slave来准备升级为master。

**从节点过滤**：判断跟master断开连接的时长

如果一个slave跟master断开连接已经超过了（down-after-milliseconds的10倍+master宕机的时长），那么slave就被认为不适合选举为master。

1. slave配置的优先级（slave-priority=100）
2. 复制offset
3. run id

说明：

1. 然后把剩下了的slave按照优先级进行排序，slave priority越低，优先级就越高。
2. 如果slave priority相同，就看replica offset，哪个slave复制了越多的数据，offset越靠后，优先级就越高。
3. 如果上面两个条件都相同，那么选择一个run id比较小的那个slave。

#### slave配置的自动纠正

哨兵会自动纠正slave的配置信息。比如某台slave要成为潜在的master候选人，哨兵会确保slave在复制现有的master数据；如果slave连接到了一个错误的master上，比如故障转移后，哨兵会确保它们连接到正确的master上。

#### quorum和majority

每次一个哨兵要做主备切换，首先需要quorum数量的哨兵认为odown，然后选举出一个哨兵来做切换，这个哨兵还要得到majority哨兵的授权，才能正式执行切换。

如果quorum < majority，比如5个哨兵，majority就是3，quorum设置为2，可以执行切换。

如果quorum >= majority，比如5个哨兵，设置quorum是5，那么必须5个哨兵都授权才能切换。

#### configuration epoch

哨兵进行切换之前，执行切换的那个哨兵从要切换到新的master那里得到一个configuration epoch，这就是一个version号，每次切换的version号都必须是唯一的。如果第一次master切换失败了，那么其他哨兵会等待failover-timeout时间，然后继续执行切换，此时会重新获得一个新的configuration epoch，作为新的version号。

#### configuration传播

哨兵完成切换之后，会在自己本地更新生成最新的master配置，然后同步给其他哨兵，通过pub/sub消息机制。

这里version号就很重要了，因为各种消息都是通过一个channel去发布和监听的，所以一个哨兵完成一次新的切换之后，新的master配置是跟着新的version号的，其他的哨兵都是根据版本号的大小来更新自己的master配置。

------

## 异步复制和集群脑裂导致数据丢失

场景一：异步复制导致的数据丢失

client往redis master写入数据，master还没来得及复制给slave，此时，master宕机了，哨兵检测到master宕机后，从slave中投票选举出新的master，但是没来得及复制的数据就丢失了。

场景二：集群脑裂导致的数据丢失

master出现了网络异常，与其他slave节点失去联系，但没有挂掉，其他slave节点上的哨兵机制重新选举了master，不过此时的client跟旧的master网络是好的，发送了数据到旧的master数据没有得到同步，此时检测到该master有问题，修好网络后作为slave挂在到新的master节点上，但是新的master没有同步网络异常时的数据导致丢失。

**如何降低损失？**

- min-slaves-to-write 1
- min-slaves-max-lag 10

配置说明：要求至少有一个slave，数据复制和同步的延迟不能超过10秒。也就是说一旦所有的slave数据的复制和同步都超过了10秒，这时master就不会再接收写请求了，可以把数据落差保持在可控范围内，减少数据损失。

上述配置也适合脑裂，master发生脑裂以后，所有slave都不向旧的master发送数据，10秒之后旧master停止接收写请求，同样也可以减少脑裂带来的数据丢失。

------

## 分布式集群-redis cluster

1. 自动将数据进行分片，每个master上放一部分数据。
2. 提供内置的高可用支持，部分master不可用时，还是可以继续工作的。

redis cluster可以突破单机redis在海量数据面前的瓶颈。

**redis cluster VS replication+sentinal**

如果数据量很少，只有几个G，主要是承载高并发性能的场景，那么单机足够了。采用主从架构，再搭建一个sentinal集群，保证高可用。如果你的数据量很大，建议使用redis cluster。

### 分布式数据存储的核心算法

#### 最老土的hash算法和弊端

把请求的数据进行hash运算，对hash值取模（针对master数量）然后放入对应的master节点中，如果某台master宕机了，该节点中的缓存数据就会失效，更严重的是由于master数量少了，导致取模方式改变，新的请求通过取模运算后得不到有效缓存，会造成几乎100%的请求涌入数据库重新生成缓存，这里会涉及到**大量的缓存重建**，这是致命的。

#### 一致性hash算法(自动缓存迁移)

有请求过来以后，同样是把key进行hash运算，然后会把hash值对应在圆环的各个点上，key落在圆环上以后就会顺时针旋转去寻找距离自己最近的master节点，如果任何一个master节点宕机，只有在该master上的缓存会失效，比如有3台master节点，宕机一台，1/3的数据流量会瞬间涌入数据库，重新查询一次，在环上的master节点越多，宕机后失效的数据越少。这只是均匀分布的情况，如有区间存在缓存热点，还是会有弊端。

#### 一致性hash算法+虚拟节点

基于一致性hash算法，在各个master节点之间，再创建均匀分布的虚拟节点，在每个区间内，大量的数据都会均匀的分布到不同的节点，不会存在大量的缓存顺时针同时融入一个master内，实现了自动的**负载均衡**。

#### hash slot算法

redis cluster有固定的16384个哈希槽，对每个key计算CRC16的值，然后对16384取模，注意不是对机器取模，所以即使有任何一台机器宕机，其他master中的缓存是不受影响的，经过短暂的数据迁移后，会把宕机中的缓存数据均匀分布到其他的master中继续提供服务。而且master slot让node的增加和移除变得很简单，只需要针对机器的个数均匀分配16384个哈希槽就可以了。

------

### redis集群模式的工作原理

#### 节点间的内部通信机制

**基础通信原理**

redis cluster节点间采用**gossip**协议进行通信。

跟集中式不同，不是将元数据（节点信息、故障等）集中存储在某个节点上，而是互相之间不断通信，保持整个集群所有节点的数据是完整的。

- 集中式：好处在于，元数据的更新和读取的时效性非常好，一旦有变更，其他节点立刻就能感知到。缺点是所有的元数据的更新全部集中在一个地方，可能导致元数据的存储压力。
- gossip：好处在于，元数据的更新比较分散，更新请求会陆陆续续的打到所有节点上去更新，降低了压力。缺点是元数据的更新有一定的延迟，可能导致集群的一些操作滞后。

10000端口：每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如7001，那么用于节点间通信的就是17001端口，每个节点每隔一段时间都会往另外几个节点发送ping信息，收到后返回pong。通过通信端口结合gossip协议相互交换信息，包括故障信息、节点的增加和删除、hash slot信息等等。

**gossip协议**

gossip协议包含多种消息，包括ping、pong、meet、fail等。

- meet：某个节点发送meet给新加入的节点，让新节点加入集群中，然后新节点就会开始和其他节点进行通信。
- ping：每个节点都会频繁的给其他节点发送ping，其中包含自己的状态还有自己维护集群的元数据，互相通过ping进行元数据的交换和更新。
- pong：返回ping和meet，包含自己的状态和其他信息，也可以用于广播和更新。
- fail：某个节点判断另一个节点fail后，就发送fail给其他节点，通知其他节点，指定的节点宕机了。

**ping消息深入**

每个节点每秒会执行10次ping，每次会选择5个最久没有通信的其他节点。如果发现某个节点通信延时达到了cluster_node_timeout，就会立即发送ping，避免数据交换延迟过长。所以cluster_node_timeout可以调节，如果调节比较大，就可以降低发送的频率。每次ping需要带上自己的节点信息，还有就是带上1/10的其他节点信息，发送出去，进行数据交换。至少包含3个其他节点的信息，最多包含（总节点-2）个其他节点信息。

------

jedis的运行原理：重定向，计算hash slot，采用smart jedis，在本地维护了一个hash slot -> node的映射表缓存。

#### 高性能与主备切换原理

redis cluster的高可用原理，几乎和哨兵是一样的。

1. 判断节点宕机

   如果一个节点认为另外一个节点宕机了，就是pfail，主观宕机。如果多个节点都认为另外一个节点pfail了，那么就是客观宕机fail。节点之间把pfail放在gossip ping中进行通信，超过半数认为pfail就是fail。

2. 从节点过滤

   对于宕机的master，从其所有的从节点slave中选择一个切换成master，检查每个slave与宕机的master断开连接的时间，如果超过了（cluster-node-timeout * cluster-slave-validity-factor）将失去选举资格。

3. master选举

   每个从节点都根据自己对master复制数据的offset，来设置一个选举时间，offset越大（复制数据越多），选举时间越靠前，优先进行选举。然后开始进行投票，具体过程和哨兵类似，选举通过成为新的master。

综上所述：redis cluster功能强大，直接集成了replication和sentinal的功能。

------

## 缓存雪崩、缓存穿透、缓存击穿

### 缓存雪崩

定义：指在某一时间段，缓存集体失效。

#### 造成缓存雪崩的原因

1. 比如双11零点抢购，大量商品被集中放入到缓存，假设缓存时效为一小时，那么到了凌晨1点的时候缓存就集体失效了，大量的请求会打在数据库上，对数据库来说，就会产生周期性的压力波峰，可能造成缓存雪崩。
2. 缓存服务节点的宕机，对数据库服务器造成的压力是不可预知的，很有可能瞬间就把数据库压垮。

#### 如何预防缓存雪崩？

针对缓存集体失效：如果是电商项目，一般是采取不同分类商品，缓存不同周期。在同一分类中的商品，加上一个随机因子。这样能尽可能分散缓存过期时间，而且，热门类目的商品缓存时间长一些，冷门类目的商品缓存时间短一些，也能节省缓存服务的资源。

针对缓存服务器宕机：redis高可用（主从+哨兵 或者 redis cluster），避免全盘奔溃。

#### 缓存雪崩了如何恢复？

本地ehcache缓存+hystrix限流&降级，避免mysql被打死。事后通过redis持久化快速恢复缓存数据。

------

### 缓存穿透

定义：是指查询一个数据库一定不存在的数据，请求穿过了缓存，直接打在了数据库。

#### 造成缓存穿透的原因

代码bug或者恶意攻击。

#### 如何预防缓存穿透？

如果从数据库查询的对象为空，也放入缓存，只是设定的缓存过期时间较短，比如设置为60秒。

------

### 缓存击穿

定义：是指一个key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个屏障上凿开了一个洞。

### 如何预防缓存击穿？

这种情况遇到的比较少，真有这种爆款key，设置成永不过期就可以了。

------

## 如何保证缓存与数据库的双写一致性

### 先删缓存，再更新数据库

最经典的缓存+数据库读写的模式：暂存模式（cache aside pattern）

1. 读的时候先读缓存，缓存没有的话就读数据库，然后把数据库的数据放到缓存，同时返回相应。
2. 更新的时候，**先删除缓存，再更新数据库**，如果修改数据库失败了，那么数据库中是旧数据，缓存中是空的，不会造成不一致，因为读的时候缓存中没有，还会把旧的数据库数据更新到缓存中。反过来，如果先更改数据库再删除缓存，如果缓存删除失败了，则会导致数据不一致。

为什么是删除缓存，而不是更新缓存呢？

原因很简单，很多时候复杂点的缓存场景，不单单是修改了一个值那么简单，而是需要结合多张表去计算才能得到缓存结果，就算真的是简单场景的缓存，也需要看看这个缓存是不是被频繁的使用到，否则只是增加麻烦而已。

其实删除缓存就是一个lazy计算的思想，不需要每次都做复杂的运算，它被用到的时候再计算就好了。

------

### 高并发场景下的数据不一致

#### 问题描述

数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没有修改，一个请求过来，去读缓存，发现缓存空了，去查询数据库，把旧的数据放到了缓存中，然后数据库完成了修改，此时数据库和缓存的数据不一致了。

#### 解决方案

把数据库与缓存的更新读取操作进行异步串行化。

- 更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个内存队列中。
- 读取数据的时候，如果发现数据不在缓存中，那么将进行（重新读取+更新缓存）操作，也根据唯一标识路由并发送到同一个内存队列中。

一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行。

这样的话，一个数据变更的操作，先删除缓存，然后去更新数据库，但是还没有完成更新；此时如果一个读请求过来，读到了空的缓存，可以先将缓存更新的请求发送到队列中积压，然后同步等待缓存更新完成。

这里有一个优化点，一个队列中，多个读请求（更新缓存）串在一起是没有意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，就不用再放更新操作进入队列了，直接等待前面的缓存更新完成即可。如果请求还在等待范围内，不断轮询发现可以取到值了就返回，超过等待时长就返回数据库中的旧值。

#### 解决方案需要注意的问题

**读请求 长时间堵塞**

由于读请求做了非常轻度的异步化，所以一定要注意读超时问题，每个读请求必须在超时时间范围内返回。

该方案的最大风险在于可能数据更新很频繁，或者包含了对多个数据项的修改，导致队列中积压了大量的更新操作在里面，然后读请求发生了大量的超时，最后导致大量的读请求直接走数据库。一定要提前做好压力测试和真实数据模拟，不过一般来说数据的写频率是很低的，所以队列中积压的应该不会太多。如果真的导致积压过多的话，可以采取增加内存队列的方式来解决。

**读请求 并发量过高**

上述方案有可能突然大量的读请求在几十毫秒内hang在服务器上，看需要几台服务器才能扛得住，所以要计算好每个读请求不要hang太久。

**多服务实例部署的请求路由**

可能这个服务部署了多个实例，那么必须保证，执行数据更新操作以及执行缓存更新操作的请求，都通过nginx服务器路由到相同的服务器实例上。

**热点商品的路由问题**

万一某个商品的读写请求特别高，全部打到相同的机器的相同队列里去了，可能造成某台机器的压力过大。

因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以更新频率不是太高的话，这个问题影响不是很大，不过的缺可能某些机器的负载高一些。

------

## 如何保证redis并发竞争的数据一致性

客户端角度：为保证每个客户端间正常有序与Redis进行通信，对连接进行池化，同时对客户端读写Redis操作采用内部锁synchronized。

服务器角度：采用分布式锁，确保同一时刻只能有一个系统实例在操作某个key，获得分布式锁以后，每次要写之前，先判断当前这个value的时间戳是否比缓存中的时间戳更新，如果更新，可以写入；否则，就不能用旧数据覆盖新数据。